{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18831b05",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160299a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T11:05:06.807229Z",
     "iopub.status.busy": "2024-04-30T11:05:06.806849Z",
     "iopub.status.idle": "2024-04-30T11:05:08.521449Z",
     "shell.execute_reply": "2024-04-30T11:05:08.519816Z",
     "shell.execute_reply.started": "2024-04-30T11:05:06.807190Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import logging\n",
    "import ipynb\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56467a7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T11:05:08.524368Z",
     "iopub.status.busy": "2024-04-30T11:05:08.523886Z",
     "iopub.status.idle": "2024-04-30T11:05:08.530563Z",
     "shell.execute_reply": "2024-04-30T11:05:08.529509Z",
     "shell.execute_reply.started": "2024-04-30T11:05:08.524345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_type_dict = {\n",
    "    \"VAE\":\"multiple\",\n",
    "    \"MLP\":\"single\",\n",
    "    \"NCF\": \"single\"}\n",
    "\n",
    "num_users_dict = {\n",
    "    \"ML1M\":6037,\n",
    "    \"Yahoo\":13797, \n",
    "    \"Pinterest\":19155}\n",
    "\n",
    "num_items_dict = {\n",
    "    \"ML1M\":3381,\n",
    "    \"Yahoo\":4604, \n",
    "    \"Pinterest\":9362}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2a2c01f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T11:07:44.793189Z",
     "iopub.status.busy": "2024-04-30T11:07:44.792730Z",
     "iopub.status.idle": "2024-04-30T11:07:44.802192Z",
     "shell.execute_reply": "2024-04-30T11:07:44.801100Z",
     "shell.execute_reply.started": "2024-04-30T11:07:44.793151Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_name = \"ML1M\" ### Can be ML1M, Yahoo, Pinterest\n",
    "recommender_name = \"NCF\" ## Can be MLP, VAE, NCF\n",
    "\n",
    "DP_DIR = Path(\"processed_data\", data_name) \n",
    "export_dir = Path(os.getcwd()).parent\n",
    "files_path = Path(export_dir, DP_DIR)\n",
    "checkpoints_path = Path(export_dir, \"check\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "output_type = output_type_dict[recommender_name] ### Can be single, multiple\n",
    "num_users = num_users_dict[data_name] \n",
    "num_items = num_items_dict[data_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12301f4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T11:07:44.986156Z",
     "iopub.status.busy": "2024-04-30T11:07:44.985782Z",
     "iopub.status.idle": "2024-04-30T11:07:44.996670Z",
     "shell.execute_reply": "2024-04-30T11:07:44.995725Z",
     "shell.execute_reply.started": "2024-04-30T11:07:44.986120Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.help_functions import *\n",
    "importlib.reload(ipynb.fs.defs.help_functions)\n",
    "from ipynb.fs.defs.help_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a6b87",
   "metadata": {},
   "source": [
    "## Data imports and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e1b88a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T11:07:56.747103Z",
     "iopub.status.busy": "2024-04-30T11:07:56.746705Z",
     "iopub.status.idle": "2024-04-30T11:07:58.221761Z",
     "shell.execute_reply": "2024-04-30T11:07:58.220470Z",
     "shell.execute_reply.started": "2024-04-30T11:07:56.747065Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(Path(files_path,f'train_data_{data_name}.csv'), index_col=0)\n",
    "test_data = pd.read_csv(Path(files_path,f'test_data_{data_name}.csv'), index_col=0)\n",
    "static_test_data = pd.read_csv(Path(files_path,f'static_test_data_{data_name}.csv'), index_col=0)\n",
    "with open(Path(files_path,f'pop_dict_{data_name}.pkl'), 'rb') as f:\n",
    "    pop_dict = pickle.load(f)\n",
    "    \n",
    "train_array = train_data.to_numpy()\n",
    "test_array = test_data.to_numpy()\n",
    "items_array = np.eye(num_items)\n",
    "all_items_tensor = torch.Tensor(items_array).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29108508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T11:07:58.224438Z",
     "iopub.status.busy": "2024-04-30T11:07:58.223895Z",
     "iopub.status.idle": "2024-04-30T11:07:58.352714Z",
     "shell.execute_reply": "2024-04-30T11:07:58.351754Z",
     "shell.execute_reply.started": "2024-04-30T11:07:58.224413Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row in range(static_test_data.shape[0]):\n",
    "    static_test_data.iloc[row, static_test_data.iloc[row,-2]]=0\n",
    "test_array = static_test_data.iloc[:,:-2].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0abdd7e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T11:07:58.353739Z",
     "iopub.status.busy": "2024-04-30T11:07:58.353577Z",
     "iopub.status.idle": "2024-04-30T11:07:58.359290Z",
     "shell.execute_reply": "2024-04-30T11:07:58.358242Z",
     "shell.execute_reply.started": "2024-04-30T11:07:58.353723Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pop_array = np.zeros(len(pop_dict))\n",
    "for key, value in pop_dict.items():\n",
    "    pop_array[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2db85e",
   "metadata": {},
   "source": [
    "# Recommenders Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b959e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.recommenders_architecture import *\n",
    "importlib.reload(ipynb.fs.defs.recommenders_architecture)\n",
    "from ipynb.fs.defs.recommenders_architecture import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc79dc",
   "metadata": {},
   "source": [
    "# Define the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fd51e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kw_dict = {'device':device,\n",
    "          'num_items': num_items,\n",
    "           'num_features': num_items, \n",
    "           'demographic':False,\n",
    "          'pop_array':pop_array,\n",
    "          'all_items_tensor':all_items_tensor,\n",
    "          'static_test_data':static_test_data,\n",
    "          'items_array':items_array,\n",
    "          'output_type':output_type,\n",
    "          'recommender_name':recommender_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771d610",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b818020",
   "metadata": {},
   "source": [
    "## MLP Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade5586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses_dict = {}\n",
    "test_losses_dict = {}\n",
    "HR10_dict = {}\n",
    "\n",
    "def MLP_objective(trial):\n",
    "    \n",
    "    lr = trial.suggest_float('learning_rate', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024])\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256, 512])\n",
    "    beta = trial.suggest_float('beta', 0, 4)\n",
    "    epochs = 10\n",
    "    model = MLP(hidden_dim, **kw_dict)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    hr10 = []\n",
    "    print(f'======================== new run - {recommender_name} ========================')\n",
    "    logger.info(f'======================== new run - {recommender_name} ========================')\n",
    "    \n",
    "    num_training = train_data.shape[0]\n",
    "    num_batches = int(np.ceil(num_training / batch_size))\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_matrix = sample_indices(train_data.copy(), **kw_dict)\n",
    "        perm = np.random.permutation(num_training)\n",
    "        loss = []\n",
    "        train_pos_loss=[]\n",
    "        train_neg_loss=[]\n",
    "        if epoch!=0 and epoch%10 == 0:\n",
    "            lr = 0.1*lr\n",
    "            optimizer.lr = lr\n",
    "        \n",
    "        for b in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "            if (b + 1) * batch_size >= num_training:\n",
    "                batch_idx = perm[b * batch_size:]\n",
    "            else:\n",
    "                batch_idx = perm[b * batch_size: (b + 1) * batch_size]    \n",
    "            batch_matrix = torch.FloatTensor(train_matrix[batch_idx,:-2]).to(device)\n",
    "\n",
    "            batch_pos_idx = train_matrix[batch_idx,-2]\n",
    "            batch_neg_idx = train_matrix[batch_idx,-1]\n",
    "            \n",
    "            batch_pos_items = torch.Tensor(items_array[batch_pos_idx]).to(device)\n",
    "            batch_neg_items = torch.Tensor(items_array[batch_neg_idx]).to(device)\n",
    "            \n",
    "            pos_output = torch.diagonal(model(batch_matrix, batch_pos_items))\n",
    "            neg_output = torch.diagonal(model(batch_matrix, batch_neg_items))\n",
    "            \n",
    "            pos_loss = torch.mean((torch.ones_like(pos_output)-pos_output)**2)\n",
    "            neg_loss = torch.mean((neg_output)**2)\n",
    "            \n",
    "            batch_loss = pos_loss + beta*neg_loss\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss.append(batch_loss.item())\n",
    "            train_pos_loss.append(pos_loss.item())\n",
    "            train_neg_loss.append(neg_loss.item())\n",
    "            \n",
    "        print(f'train pos_loss = {np.mean(train_pos_loss)}, neg_loss = {np.mean(train_neg_loss)}')    \n",
    "        train_losses.append(np.mean(loss))\n",
    "        torch.save(model.state_dict(), Path(checkpoints_path, f'MLP_{data_name}_{round(lr,4)}_{batch_size}_{trial.number}_{epoch}.pt'))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        test_matrix = np.array(static_test_data)\n",
    "        test_tensor = torch.Tensor(test_matrix[:,:-2]).to(device)\n",
    "        \n",
    "        test_pos = test_matrix[:,-2]\n",
    "        test_neg = test_matrix[:,-1]\n",
    "        \n",
    "        row_indices = np.arange(test_matrix.shape[0])\n",
    "        test_tensor[row_indices,test_pos] = 0\n",
    "        \n",
    "        pos_items = torch.Tensor(items_array[test_pos]).to(device)\n",
    "        neg_items = torch.Tensor(items_array[test_neg]).to(device)\n",
    "        \n",
    "        pos_output = torch.diagonal(model(test_tensor, pos_items).to(device))\n",
    "        neg_output = torch.diagonal(model(test_tensor, neg_items).to(device))\n",
    "        \n",
    "        pos_loss = torch.mean((torch.ones_like(pos_output)-pos_output)**2)\n",
    "        neg_loss = torch.mean((neg_output)**2)\n",
    "        print(f'test pos_loss = {pos_loss}, neg_loss = {neg_loss}')\n",
    "        \n",
    "        hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR = recommender_evaluations(model, **kw_dict)\n",
    "        hr10.append(hit_rate_at_10)\n",
    "        print(hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR)\n",
    "        \n",
    "        test_losses.append(-hit_rate_at_10)\n",
    "        if epoch>5:\n",
    "            if test_losses[-2]<=test_losses[-1] and test_losses[-3]<=test_losses[-2] and test_losses[-4]<=test_losses[-3]:\n",
    "                logger.info(f'Early stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "                train_losses_dict[trial.number] = train_losses\n",
    "                test_losses_dict[trial.number] = test_losses\n",
    "                HR10_dict[trial.number] = hr10\n",
    "                return max(hr10)\n",
    "            \n",
    "    logger.info(f'Stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "    train_losses_dict[trial.number] = train_losses\n",
    "    test_losses_dict[trial.number] = test_losses\n",
    "    HR10_dict[trial.number] = hr10\n",
    "    return max(hr10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc5616",
   "metadata": {},
   "source": [
    "## VAE Train function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826593a-e763-4776-90a0-94be8c4d5b9c",
   "metadata": {},
   "source": [
    "### Define the configs (they are defined once again inside the load recommender function in the \"help funcion\" notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f8609-1f73-47db-8923-763274c726a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_config= {\n",
    "\"enc_dims\": [512,128],\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000}\n",
    "\n",
    "\n",
    "Pinterest_VAE_config= {\n",
    "\"enc_dims\": [256,64],\n",
    "\"dropout\": 0.5,\n",
    "\"anneal_cap\": 0.2,\n",
    "\"total_anneal_steps\": 200000}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d9369-4c29-4ea5-876f-d770dfbaffaa",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68648a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses_dict = {}\n",
    "test_losses_dict = {}\n",
    "HR10_dict = {}\n",
    "\n",
    "\n",
    "def VAE_objective(trial):\n",
    "    \n",
    "    lr = trial.suggest_float('learning_rate', 0.001, 0.01)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64,128,256])\n",
    "    epochs = 20\n",
    "    if data_name == \"Pinterest\":\n",
    "        model = VAE(Pinterest_VAE_config ,**kw_dict)\n",
    "    else:\n",
    "        model = VAE(VAE_config ,**kw_dict)\n",
    "    else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    hr10 = []\n",
    "    print('======================== new run ========================')\n",
    "    logger.info('======================== new run ========================')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if epoch!=0 and epoch%10 == 0:\n",
    "            lr = 0.1*lr\n",
    "            optimizer.lr = lr\n",
    "        loss = model.train_one_epoch(train_array, optimizer, batch_size)\n",
    "        train_losses.append(loss)\n",
    "        torch.save(model.state_dict(), Path(checkpoints_path, f'VAE_{data_name}_{trial.number}_{epoch}_{round(lr,4)}_{batch_size}.pt'))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        test_matrix = static_test_data.to_numpy()\n",
    "        test_tensor = torch.Tensor(test_matrix[:,:-2]).to(device)\n",
    "        test_pos = test_array[:,-2]\n",
    "        test_neg = test_array[:,-1]\n",
    "        row_indices = np.arange(test_matrix.shape[0])\n",
    "        test_tensor[row_indices,test_pos] = 0\n",
    "        output = model(test_tensor).to(device)\n",
    "        pos_loss = -output[row_indices,test_pos].mean()\n",
    "        neg_loss = output[row_indices,test_neg].mean()\n",
    "        print(f'pos_loss = {pos_loss}, neg_loss = {neg_loss}')\n",
    "        \n",
    "        hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR = recommender_evaluations(model, **kw_dict)\n",
    "        hr10.append(hit_rate_at_10)\n",
    "        print(hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR)\n",
    "        \n",
    "        test_losses.append(pos_loss.item())\n",
    "        if epoch>5:\n",
    "            if test_losses[-2]<test_losses[-1] and test_losses[-3]<test_losses[-2] and test_losses[-4]<test_losses[-3]:\n",
    "                logger.info(f'Early stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "                train_losses_dict[trial.number] = train_losses\n",
    "                test_losses_dict[trial.number] = test_losses\n",
    "                HR10_dict[trial.number] = hr10\n",
    "                return max(hr10)\n",
    "    \n",
    "    logger.info(f'Stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "    train_losses_dict[trial.number] = train_losses\n",
    "    test_losses_dict[trial.number] = test_losses\n",
    "    HR10_dict[trial.number] = hr10\n",
    "    return max(hr10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d568bc",
   "metadata": {},
   "source": [
    "## NCF training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a838f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses_dict = {}\n",
    "test_losses_dict = {}\n",
    "HR10_dict = {}\n",
    "\n",
    "## PAY ATTENTION to define manualy the MLP_model and GMF_model checkpoints which will be used inside the NCF\n",
    "\n",
    "def NCF_objective(trial):\n",
    "    lr = trial.suggest_float('learning_rate', 0.0005, 0.005)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32,64,128])\n",
    "    beta = trial.suggest_float('beta',0, 4)\n",
    "    epochs = 20\n",
    "    MLP = MLP_model(hidden_size=8, num_layers=3, **kw_dict)\n",
    "    # EDIT HERE\n",
    "    MLP_checkpoint = torch.load(Path(checkpoints_path, 'MLP_model_ML1M_0.0001_64_27.pt'))\n",
    "    MLP.load_state_dict(MLP_checkpoint)\n",
    "    MLP.train()\n",
    "    GMF = GMF_model(hidden_size=8, **kw_dict)\n",
    "    # & EDIT HERE\n",
    "    GMF_checkpoint = torch.load(Path(checkpoints_path, 'GMF_best_ML1M_0.0001_32_17.pt'))\n",
    "    GMF.load_state_dict(GMF_checkpoint)\n",
    "    GMF.train()\n",
    "    model = NCF(factor_num=8, num_layers=3, dropout=0.5, model= 'NeuMF-pre', GMF_model= GMF, MLP_model=MLP, **kw_dict)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    hr10 = []\n",
    "    print(f'======================== new run - {recommender_name} ========================')\n",
    "    logger.info(f'======================== new run - {recommender_name} ========================')\n",
    "    \n",
    "    num_training = train_data.shape[0]\n",
    "    num_batches = int(np.ceil(num_training / batch_size))\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_matrix = sample_indices(train_data.copy(), **kw_dict)\n",
    "        perm = np.random.permutation(num_training)\n",
    "        loss = []\n",
    "        train_pos_loss=[]\n",
    "        train_neg_loss=[]\n",
    "        if epoch!=0 and epoch%10 == 0:\n",
    "            lr = 0.1*lr\n",
    "            optimizer.lr = lr\n",
    "        \n",
    "        for b in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "            if (b + 1) * batch_size >= num_training:\n",
    "                batch_idx = perm[b * batch_size:]\n",
    "            else:\n",
    "                batch_idx = perm[b * batch_size: (b + 1) * batch_size]    \n",
    "            batch_matrix = torch.FloatTensor(train_matrix[batch_idx,:-2]).to(device)\n",
    "\n",
    "            batch_pos_idx = train_matrix[batch_idx,-2]\n",
    "            batch_neg_idx = train_matrix[batch_idx,-1]\n",
    "            \n",
    "            batch_pos_items = torch.Tensor(items_array[batch_pos_idx]).to(device)\n",
    "            batch_neg_items = torch.Tensor(items_array[batch_neg_idx]).to(device)\n",
    "            \n",
    "            pos_output = model(batch_matrix, batch_pos_items)\n",
    "            neg_output = model(batch_matrix, batch_neg_items)\n",
    "\n",
    "            pos_loss = -torch.log(pos_output).mean()\n",
    "            neg_loss = -torch.log(torch.ones_like(neg_output)-neg_output).mean()\n",
    "\n",
    "            batch_loss = pos_loss + beta*neg_loss\n",
    "            if batch_loss<torch.inf:\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            loss.append(batch_loss.item())\n",
    "            train_pos_loss.append(pos_loss.item())\n",
    "            train_neg_loss.append(neg_loss.item())\n",
    "            \n",
    "        print(f'train pos_loss = {np.mean(train_pos_loss)}, neg_loss = {np.mean(train_neg_loss)}')    \n",
    "        train_losses.append(np.mean(loss))\n",
    "        torch.save(model.state_dict(), Path(checkpoints_path, f'{recommender_name}2_{data_name}_{round(lr,5)}_{batch_size}_{trial.number}_{epoch}.pt'))\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        test_matrix = np.array(static_test_data)\n",
    "        test_tensor = torch.Tensor(test_matrix[:,:-2]).to(device)\n",
    "        \n",
    "        test_pos = test_matrix[:,-2]\n",
    "        test_neg = test_matrix[:,-1]\n",
    "        \n",
    "        row_indices = np.arange(test_matrix.shape[0])\n",
    "        test_tensor[row_indices,test_pos] = 0\n",
    "        \n",
    "        pos_items = torch.Tensor(items_array[test_pos]).to(device)\n",
    "        neg_items = torch.Tensor(items_array[test_neg]).to(device)\n",
    "        \n",
    "        pos_output = model(test_tensor, pos_items).to(device)\n",
    "        neg_output = model(test_tensor, neg_items).to(device)\n",
    "        \n",
    "        pos_loss = -torch.log(pos_output).mean()\n",
    "        neg_loss = -torch.log(torch.ones_like(neg_output)-neg_output).mean()\n",
    "        print(f'test pos_loss = {pos_loss}, neg_loss = {neg_loss}')\n",
    "        \n",
    "        hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR = recommender_evaluations(model, **kw_dict)\n",
    "        hr10.append(hit_rate_at_10)\n",
    "        print(hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR)\n",
    "                   \n",
    "        \n",
    "        test_losses.append(-hit_rate_at_10)\n",
    "        if epoch>5:\n",
    "            if test_losses[-2]<=test_losses[-1] and test_losses[-3]<=test_losses[-2] and test_losses[-4]<=test_losses[-3]:\n",
    "                logger.info(f'Early stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "                train_losses_dict[trial.number] = train_losses\n",
    "                test_losses_dict[trial.number] = test_losses\n",
    "                HR10_dict[trial.number] = hr10\n",
    "                return max(hr10)\n",
    "            \n",
    "    logger.info(f'Stop at trial with batch size = {batch_size} and lr = {lr}. Best results at epoch {np.argmin(test_losses)} with value {np.min(test_losses)}')\n",
    "    train_losses_dict[trial.number] = train_losses\n",
    "    test_losses_dict[trial.number] = test_losses\n",
    "    HR10_dict[trial.number] = hr10\n",
    "    return max(hr10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b4d42",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "logger.setLevel(logging.INFO)  # Setup the root logger.\n",
    "logger.addHandler(logging.FileHandler(f\"{recommender_name}_{data_name}_Optuna.log\", mode=\"w\"))\n",
    "\n",
    "optuna.logging.enable_propagation()  # Propagate logs to the root logger.\n",
    "optuna.logging.disable_default_handler()  # Stop showing logs in sys.stderr.\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "logger.info(\"Start optimization.\")\n",
    "study.optimize(NCF_objective, n_trials=20)\n",
    "\n",
    "with open(f\"{recommender_name}_{data_name}_Optuna.log\") as f:\n",
    "    assert f.readline().startswith(\"A new study created\")\n",
    "    assert f.readline() == \"Start optimization.\\n\"\n",
    "    \n",
    "    \n",
    "# Print best hyperparameters and corresponding metric value\n",
    "print(\"Best hyperparameters: {}\".format(study.best_params))\n",
    "print(\"Best metric value: {}\".format(study.best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d135397",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in HR10_dict.keys():\n",
    "    print(run, np.argmax(HR10_dict[run]), max(HR10_dict[run]))\n",
    "    plt.plot(HR10_dict[run])\n",
    "plt.legend(HR10_dict.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82576ff3",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e5880",
   "metadata": {},
   "source": [
    "## Load the trained recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a25ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_path_dict = {\n",
    "    (\"ML1M\",\"VAE\"): Path(checkpoints_path, \"VAE_ML1M_0.0007_128_10.pt\"),\n",
    "    (\"ML1M\",\"MLP\"):Path(checkpoints_path, \"MLP1_ML1M_0.0076_256_7.pt\"),\n",
    "    (\"ML1M\",\"NCF\"):Path(checkpoints_path, \"NCF_ML1M_5e-05_64_16.pt\"),\n",
    "\n",
    "    (\"Yahoo\",\"VAE\"): Path(checkpoints_path, \"VAE_Yahoo_0.0001_128_13.pt\"),\n",
    "    (\"Yahoo\",\"MLP\"):Path(checkpoints_path, \"MLP2_Yahoo_0.0083_128_1.pt\"),\n",
    "    (\"Yahoo\",\"NCF\"):Path(checkpoints_path, \"NCF_Yahoo_0.001_64_21_0.pt\"),\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): Path(checkpoints_path, \"VAE_Pinterest_12_18_0.0001_256.pt\"),\n",
    "    (\"Pinterest\",\"MLP\"):Path(checkpoints_path, \"MLP_Pinterest_0.0062_512_21_0.pt\"),\n",
    "    (\"Pinterest\",\"NCF\"):Path(checkpoints_path, \"NCF2_Pinterest_9e-05_32_9_10.pt\")}\n",
    "\n",
    "\n",
    "hidden_dim_dict = {\n",
    "    (\"ML1M\",\"VAE\"): None,\n",
    "    (\"ML1M\",\"MLP\"): 32,\n",
    "    (\"ML1M\",\"NCF\"): 8,\n",
    "    \n",
    "    (\"Yahoo\",\"VAE\"): None,\n",
    "    (\"Yahoo\",\"MLP\"):32,\n",
    "    (\"Yahoo\",\"NCF\"):8,\n",
    "    \n",
    "    (\"Pinterest\",\"VAE\"): None,\n",
    "    (\"Pinterest\",\"MLP\"):512,\n",
    "    (\"Pinterest\",\"NCF\"): 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b061a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = hidden_dim_dict[(data_name, recommender_name)]\n",
    "recommender_path = recommender_path_dict[(data_name, recommender_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652def72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_recommender(data_name, hidden_dim, checkpoints_path, recommender_path, **kw_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e57f7",
   "metadata": {},
   "source": [
    "## plot the distribution of top recommended item accross all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ceeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of top recommended item accross all users\n",
    "topk_train = {}\n",
    "for i in range(len(train_array)):\n",
    "    vec = train_array[i]\n",
    "    tens = torch.Tensor(vec).to(device)\n",
    "    topk_train[i] = int(get_user_recommended_item(tens, model).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(topk_train.values(), bins=1000)\n",
    "plt.plot(np.array(list(pop_dict.keys())), np.array(list(pop_dict.values()))*100, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_test = {}\n",
    "for i in range(len(test_array)):\n",
    "    vec = test_array[i]\n",
    "    tens = torch.Tensor(vec).to(device)\n",
    "    topk_test[i] = int(get_user_recommended_item(tens, model).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(topk_test.values(), bins=400)\n",
    "plt.plot(np.array(list(pop_dict.keys())), np.array(list(pop_dict.values()))*200, alpha=0.2)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9637160",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR = recommender_evaluations(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ae07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hit_rate_at_10, hit_rate_at_50, hit_rate_at_100, MRR, MPR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
