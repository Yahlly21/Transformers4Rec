{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9956ce",
   "metadata": {},
   "source": [
    "### This notebook presents the architectures of the three recommendation systems tested within this framework\n",
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6583e643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T09:52:39.172992Z",
     "iopub.status.busy": "2024-06-19T09:52:39.172160Z",
     "iopub.status.idle": "2024-06-19T09:52:40.391566Z",
     "shell.execute_reply": "2024-06-19T09:52:40.390429Z",
     "shell.execute_reply.started": "2024-06-19T09:52:39.172952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "export_dir = os.getcwd()\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import optuna\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd66af",
   "metadata": {},
   "source": [
    "# 2. MLP recommender Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb9ccf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T09:53:34.594600Z",
     "iopub.status.busy": "2024-06-19T09:53:34.594263Z",
     "iopub.status.idle": "2024-06-19T09:53:34.618928Z",
     "shell.execute_reply": "2024-06-19T09:53:34.615411Z",
     "shell.execute_reply.started": "2024-06-19T09:53:34.594579Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, **kw):\n",
    "        super(MLP, self).__init__()\n",
    "        user_size = kw['num_items']\n",
    "        item_size = kw['num_items']\n",
    "        self.device = kw['device']\n",
    "        self.users_fc = nn.Linear(user_size, hidden_size, bias = True).to(self.device)\n",
    "        self.items_fc = nn.Linear(item_size, hidden_size, bias = True).to(self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        user_vec = self.users_fc(user_tensor.to(self.device))\n",
    "        item_vec = self.items_fc(item_tensor.to(self.device))\n",
    "        output = torch.matmul(user_vec, item_vec.T).to(self.device)\n",
    "        return self.sigmoid(output).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c694d",
   "metadata": {},
   "source": [
    "# 3. VAE recommender Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, model_conf, **kw):\n",
    "        super(VAE, self).__init__()\n",
    "        self.device = kw['device'] \n",
    "        num_features = kw['num_features'] \n",
    "        num_items = kw['num_items'] \n",
    "        self.demographic = kw['demographic'] \n",
    "        if self.demographic:\n",
    "            self.num_items = num_features\n",
    "            self.items_only = num_items\n",
    "        else:\n",
    "            self.num_items = num_items\n",
    "        self.enc_dims = [self.num_items] + model_conf['enc_dims']\n",
    "        self.dec_dims = self.enc_dims[::-1]\n",
    "        self.dims = self.enc_dims + self.dec_dims[1:]\n",
    "        self.dropout = model_conf['dropout']\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.total_anneal_steps = model_conf['total_anneal_steps']\n",
    "        self.anneal_cap = model_conf['anneal_cap']\n",
    "\n",
    "        self.eps = 1e-6\n",
    "        self.anneal = 0.\n",
    "        self.update_count = 0\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.enc_dims[:-1], self.enc_dims[1:])):\n",
    "            if i == len(self.enc_dims[:-1]) - 1:\n",
    "                d_out *= 2\n",
    "            self.encoder.append(nn.Linear(d_in, d_out))\n",
    "            if i != len(self.enc_dims[:-1]) - 1:\n",
    "                self.encoder.append(nn.ReLU())\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i, (d_in, d_out) in enumerate(zip(self.dec_dims[:-1], self.dec_dims[1:])):\n",
    "            self.decoder.append(nn.Linear(d_in, d_out))\n",
    "            if i != len(self.dec_dims[:-1]) - 1:\n",
    "                self.decoder.append(nn.ReLU())\n",
    "                \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, rating_matrix):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :param rating_matrix: rating matrix\n",
    "        \"\"\"\n",
    "        # encoder\n",
    "        if len(rating_matrix.shape) == 1:\n",
    "            rating_matrix = torch.unsqueeze(rating_matrix, 0)\n",
    "        h = F.dropout(F.normalize(rating_matrix, dim=-1), p=self.dropout, training=self.training)\n",
    "        for layer in self.encoder:\n",
    "            h = layer(h)\n",
    "\n",
    "        # sample\n",
    "        mu_q = h[:, :self.enc_dims[-1]]\n",
    "        logvar_q = h[:, self.enc_dims[-1]:]  # log sigmod^2  batch x 200\n",
    "        std_q = torch.exp(0.5 * logvar_q)  # sigmod batch x 200\n",
    "        \n",
    "        epsilon = torch.zeros_like(std_q).normal_(mean=0, std=0.01)\n",
    "        sampled_z = mu_q + self.training * epsilon * std_q\n",
    "\n",
    "        output = sampled_z\n",
    "        for layer in self.decoder:\n",
    "            output = layer(output)\n",
    "\n",
    "        if self.training:\n",
    "            kl_loss = ((0.5 * (-logvar_q + torch.exp(logvar_q) + torch.pow(mu_q, 2) - 1)).sum(1)).mean()\n",
    "            return output, kl_loss\n",
    "        else:\n",
    "            if self.demographic:\n",
    "                return self.softmax(output[:,:self.items_only])\n",
    "            else:\n",
    "                return self.softmax(output)   \n",
    "        \n",
    "    def train_one_epoch(self, dataset, optimizer, batch_size, alpha=0.5):\n",
    "        \"\"\"\n",
    "        Train model for one epoch\n",
    "        :param dataset: given data\n",
    "        :param optimizer: choice of optimizer\n",
    "        :param batch_size: batch size\n",
    "        :return: model loss\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "        train_matrix = dataset\n",
    "\n",
    "        num_training = train_matrix.shape[0]\n",
    "        num_batches = int(np.ceil(num_training / batch_size))\n",
    "        perm = np.random.permutation(num_training)\n",
    "\n",
    "        loss = 0.0\n",
    "        for b in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if (b + 1) * batch_size >= num_training:\n",
    "                batch_idx = perm[b * batch_size:]\n",
    "            else:\n",
    "                batch_idx = perm[b * batch_size: (b + 1) * batch_size]\n",
    "            batch_matrix = torch.FloatTensor(train_matrix[batch_idx]).to(self.device)\n",
    "\n",
    "            if self.total_anneal_steps > 0:\n",
    "                self.anneal = min(self.anneal_cap, 1. * self.update_count / self.total_anneal_steps)\n",
    "            else:\n",
    "                self.anneal = self.anneal_cap\n",
    "\n",
    "            pred_matrix, kl_loss = self.forward(batch_matrix)\n",
    "\n",
    "            # cross_entropy\n",
    "            total_ce = -(F.log_softmax(pred_matrix, 1) * batch_matrix)\n",
    "            ce_hist = total_ce[:,:self.num_items].sum(1).mean()\n",
    "            ce_demo = total_ce[:,self.num_items:].sum(1).mean()\n",
    "            ce_loss = ce_hist+alpha*ce_demo\n",
    "\n",
    "            batch_loss = ce_loss + kl_loss * self.anneal\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            self.update_count += 1\n",
    "\n",
    "            loss += batch_loss\n",
    "            if b % 200 == 0:\n",
    "                print('(%3d / %3d) loss = %.4f' % (b, num_batches, batch_loss))\n",
    "        return loss\n",
    "\n",
    "    def predict(self, eval_users, test_batch_size):\n",
    "        \"\"\"\n",
    "        Predict the model on test set\n",
    "        :param eval_users: evaluation (test) user\n",
    "        :param eval_pos: position of the evaluated (test) item\n",
    "        :param test_batch_size: batch size for test set\n",
    "        :return: predictions\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            input_matrix = torch.Tensor(eval_users).to(self.device)\n",
    "            preds = np.zeros_like(input_matrix.cpu())\n",
    "\n",
    "            num_data = input_matrix.shape[0]\n",
    "            num_batches = int(np.ceil(num_data / test_batch_size))\n",
    "            perm = list(range(num_data))\n",
    "            for b in range(num_batches):\n",
    "                if (b + 1) * test_batch_size >= num_data:\n",
    "                    batch_idx = perm[b * test_batch_size:]\n",
    "                else:\n",
    "                    batch_idx = perm[b * test_batch_size: (b + 1) * test_batch_size]\n",
    "                    \n",
    "                test_batch_matrix = input_matrix[batch_idx]\n",
    "                batch_pred_matrix = self.forward(test_batch_matrix)\n",
    "                batch_pred_matrix.masked_fill(test_batch_matrix.bool(), float('-inf'))\n",
    "                preds[batch_idx] = batch_pred_matrix.detach().cpu().numpy()\n",
    "        return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e79158",
   "metadata": {},
   "source": [
    "# 4. NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF_model(nn.Module):\n",
    "    def __init__(self, hidden_size=8, **kw):\n",
    "        super(GMF_model, self).__init__()\n",
    "        self.device = kw['device']\n",
    "        user_size = kw['num_features']\n",
    "        item_size = kw['num_items']\n",
    "        self.embed_user_GMF = nn.Linear(user_size, hidden_size, bias = False).to(self.device)\n",
    "        self.embed_item_GMF = nn.Linear(item_size, hidden_size, bias = False).to(self.device)\n",
    "        self.predict_layer = nn.Linear(hidden_size, 1, bias = True).to(self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        user_vec = self.embed_user_GMF(user_tensor.to(self.device))\n",
    "        item_vec = self.embed_item_GMF(item_tensor.to(self.device))\n",
    "        if user_vec.shape!=item_vec.shape:\n",
    "            user_res = torch.zeros(item_vec.shape).to(self.device)\n",
    "            user_res[:] = user_vec\n",
    "            user_vec = user_res\n",
    "            \n",
    "        output = self.predict_layer(torch.mul(user_vec, item_vec))\n",
    "        \n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7950c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_model(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, **kw):\n",
    "        super(MLP_model, self).__init__()\n",
    "        self.device = kw['device']\n",
    "        user_size = kw['num_features']\n",
    "        item_size = kw['num_items']\n",
    "        factor_num = hidden_size\n",
    "        self.embed_user_MLP = nn.Linear(user_size, factor_num * (2 ** (num_layers - 1)), bias = False).to(self.device)\n",
    "        self.embed_item_MLP = nn.Linear(item_size, factor_num * (2 ** (num_layers - 1)), bias = False).to(self.device)\n",
    "        \n",
    "        MLP_modules = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = factor_num * (2 ** (num_layers - i))\n",
    "            MLP_modules.append(nn.Dropout(p=0.5))\n",
    "            MLP_modules.append(nn.Linear(input_size, input_size//2).to(self.device))\n",
    "            MLP_modules.append(nn.ReLU())\n",
    "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
    "        \n",
    "        self.predict_layer = nn.Linear(hidden_size, 1, bias = True).to(self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, user_tensor, item_tensor):\n",
    "        embed_user_MLP = self.embed_user_MLP(user_tensor.to(self.device))\n",
    "        embed_item_MLP = self.embed_item_MLP(item_tensor.to(self.device))\n",
    "        if embed_user_MLP.shape!=embed_item_MLP.shape:\n",
    "            user_res = torch.zeros(embed_item_MLP.shape).to(self.device)\n",
    "            user_res[:] = embed_user_MLP\n",
    "            embed_user_MLP = user_res\n",
    "        interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
    "        output_MLP = self.MLP_layers(interaction)\n",
    "        output = self.predict_layer(output_MLP)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, factor_num, num_layers,\n",
    "                    dropout, model, GMF_model=None, MLP_model=None, **kw):\n",
    "        super(NCF, self).__init__()\n",
    "        \"\"\"\n",
    "        user_num: number of users;\n",
    "        item_num: number of items;\n",
    "        factor_num: number of predictive factors;\n",
    "        num_layers: the number of layers in MLP model;\n",
    "        dropout: dropout rate between fully connected layers;\n",
    "        model: 'MLP', 'GMF', 'NeuMF-end', and 'NeuMF-pre';\n",
    "        GMF_model: pre-trained GMF weights;\n",
    "        MLP_model: pre-trained MLP weights.\n",
    "        \"\"\"        \n",
    "        self.dropout = dropout\n",
    "        self.model = model\n",
    "        self.GMF_model = GMF_model\n",
    "        self.MLP_model = MLP_model\n",
    "        self.device = kw['device']\n",
    "        user_size = kw['num_features']\n",
    "        item_size = kw['num_items']\n",
    "        self.embed_user_GMF = nn.Linear(user_size, factor_num, bias = False).to(self.device)\n",
    "        self.embed_item_GMF = nn.Linear(item_size, factor_num, bias = False).to(self.device)\n",
    "        self.embed_user_MLP = nn.Linear(\n",
    "                user_size, factor_num * (2 ** (num_layers - 1)), bias = False).to(self.device)\n",
    "        self.embed_item_MLP = nn.Linear(\n",
    "                item_size, factor_num * (2 ** (num_layers - 1)), bias = False).to(self.device)\n",
    "\n",
    "        MLP_modules = []\n",
    "        for i in range(num_layers):\n",
    "            input_size = factor_num * (2 ** (num_layers - i))\n",
    "            MLP_modules.append(nn.Dropout(p=self.dropout))\n",
    "            MLP_modules.append(nn.Linear(input_size, input_size//2).to(self.device))\n",
    "            MLP_modules.append(nn.ReLU())\n",
    "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
    "\n",
    "        if self.model in ['MLP', 'GMF']:\n",
    "            predict_size = factor_num \n",
    "        else:\n",
    "            predict_size = factor_num * 2\n",
    "        self.predict_layer = nn.Linear(predict_size, 1).to(self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self._init_weight_()\n",
    "        \n",
    "        self.embed_user_GMF.to(self.device)\n",
    "        self.embed_item_GMF.to(self.device)\n",
    "        self.embed_user_MLP.to(self.device)\n",
    "        self.embed_item_MLP.to(self.device)\n",
    "\n",
    "    def _init_weight_(self):\n",
    "        \"\"\" We leave the weights initialization here. \"\"\"\n",
    "        if not self.model == 'NeuMF-pre':\n",
    "            nn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
    "            nn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
    "            nn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
    "            nn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
    "\n",
    "            for m in self.MLP_layers:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.kaiming_uniform_(self.predict_layer.weight, \n",
    "                                    a=1, nonlinearity='sigmoid')\n",
    "\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "        else:\n",
    "            # embedding layers\n",
    "            self.embed_user_GMF.weight.data.copy_(\n",
    "                            self.GMF_model.embed_user_GMF.weight)\n",
    "            self.embed_item_GMF.weight.data.copy_(\n",
    "                            self.GMF_model.embed_item_GMF.weight)\n",
    "            self.embed_user_MLP.weight.data.copy_(\n",
    "                            self.MLP_model.embed_user_MLP.weight)\n",
    "            self.embed_item_MLP.weight.data.copy_(\n",
    "                            self.MLP_model.embed_item_MLP.weight)\n",
    "\n",
    "            # mlp layers\n",
    "            for (m1, m2) in zip(\n",
    "                self.MLP_layers, self.MLP_model.MLP_layers):\n",
    "                if isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):\n",
    "                    m1.weight.data.copy_(m2.weight)\n",
    "                    m1.bias.data.copy_(m2.bias)\n",
    "\n",
    "            # predict layers\n",
    "            predict_weight = torch.cat([\n",
    "                self.GMF_model.predict_layer.weight, \n",
    "                self.MLP_model.predict_layer.weight], dim=1)\n",
    "            precit_bias = self.GMF_model.predict_layer.bias + \\\n",
    "                        self.MLP_model.predict_layer.bias\n",
    "\n",
    "            self.predict_layer.weight.data.copy_(0.5 * predict_weight)\n",
    "            self.predict_layer.bias.data.copy_(0.5 * precit_bias)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        if not self.model == 'MLP':\n",
    "            embed_user_GMF = self.embed_user_GMF(user)\n",
    "            embed_item_GMF = self.embed_item_GMF(item)\n",
    "            if embed_user_GMF.shape!=embed_item_GMF.shape:\n",
    "                user_res = torch.zeros(embed_item_GMF.shape).to(self.device)\n",
    "                user_res[:] = embed_user_GMF\n",
    "                embed_user_GMF = user_res\n",
    "            output_GMF = embed_user_GMF * embed_item_GMF\n",
    "        if not self.model == 'GMF':\n",
    "            embed_user_MLP = self.embed_user_MLP(user)\n",
    "            embed_item_MLP = self.embed_item_MLP(item)\n",
    "            if embed_user_MLP.shape!=embed_item_MLP.shape:\n",
    "                user_res = torch.zeros(embed_item_MLP.shape).to(self.device)\n",
    "                user_res[:] = embed_user_MLP\n",
    "                embed_user_MLP = user_res\n",
    "            interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
    "            output_MLP = self.MLP_layers(interaction)\n",
    "\n",
    "        if self.model == 'GMF':\n",
    "            concat = output_GMF\n",
    "        elif self.model == 'MLP':\n",
    "            concat = output_MLP\n",
    "        else:\n",
    "            concat = torch.cat((output_GMF, output_MLP), -1)\n",
    "\n",
    "        prediction = self.predict_layer(concat)\n",
    "        prediction = self.sigmoid(prediction)\n",
    "        return prediction.view(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
